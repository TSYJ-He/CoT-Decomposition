{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25fe7b67",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b2ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706d08a",
   "metadata": {},
   "source": [
    "# Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_model(model_path,model_type, device=\"cuda\",checkpoint_path=None):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=device,\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "                                                 )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    if model_type == \"checkpoint\":\n",
    "        checkpoints = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoints, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_test_sample(model,tokenizer,questions, max_new_tokens=2048, device=\"cuda\"):\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens+input_ids.shape[1],\n",
    "            do_sample=True,\n",
    "            top_p=0.7,\n",
    "            temperature=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    outputs = generation_output[:, input_ids.shape[1]:]\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return decoded_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47717431",
   "metadata": {},
   "source": [
    "# Prepare the checkpoint\n",
    "\n",
    "download:\n",
    "    1. raw model from modelscope\n",
    "    2. sft model from modelscope\n",
    "    3. rl checkpoint from training\n",
    "\n",
    "and place them in your local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model_path =  \"/home/fit/alex/.cache/modelscope/hub/models/Qwen/Qwen3-8B\"#\"local_path/to/your/raw/model\"\n",
    "sft_model_path =  \"/WORK/fit/alex/Kaisen/checkpoints/qwen/openr1-2/global_step_1000/huggingface\"#\"local_path/to/your/sft/model\"\n",
    "rl_model_path = \"/home/fit/alex/Kaisen.Yang/CoT Decomposition/temp_huggingface\"\n",
    "\n",
    "model_path = rl_model_path\n",
    "model_type = \"rl\"\n",
    "device= \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78244003",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=device,\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "                trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "prompt = \"Find the solution to the problem: If x^2 +x + 1 = 2, what is the value of x?\"  # 输入文本\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False )\n",
    "text += \"<EXPLORATION>\" # need to add manually for SFT and RL model\n",
    "# text+= \"<EXPLORATION></EXPLORATION>\" # This will do execusion without exploration\n",
    "input = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input[\"input_ids\"],\n",
    "        attention_mask=input[\"attention_mask\"],\n",
    "        max_new_tokens=4000,\n",
    "        do_sample=True,\n",
    "        top_p=0.7,\n",
    "        temperature=0.95,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "output = generation_output[0, input[\"input_ids\"].shape[1]:]\n",
    "decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "print(decoded_output)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
